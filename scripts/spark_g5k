#!/usr/bin/env python

import os
import sys
from argparse import ArgumentParser, RawTextHelpFormatter

from execo import Host
from execo.log import style

from execo_engine import logger

from hadoop_g5k.cluster import HadoopCluster
from hadoop_g5k.ecosystem.spark import SparkCluster, STANDALONE_MODE, YARN_MODE, \
    JavaOrScalaSparkJob, PythonSparkJob
from hadoop_g5k.util import hw_manager
from bigdata_dpy.util import generate_new_id, \
    get_default_id, deserialize_cluster, remove_cluster, serialize_cluster, \
    cluster_exists, link_to_hadoop_cluster


def parse_args():

    prog = "spark_g5k"
    description = "This tool helps you to manage a Spark cluster in Grid5000."
    parser = ArgumentParser(prog=prog,
                            description=description,
                            formatter_class=RawTextHelpFormatter,
                            add_help=False)

    actions = parser.add_argument_group(style.host("General options"),
                                        "Options to be used generally "
                                        "with Spark actions.")

    actions.add_argument("-h", "--help",
                         action="help",
                         help="Show this help message and exit")

    actions.add_argument("--id",
                         action="store",
                         nargs=1,
                         metavar="ID",
                         help="The identifier of the cluster. If not indicated"
                                ", last used cluster will be used (if any)")

    verbose_group = actions.add_mutually_exclusive_group()

    verbose_group.add_argument("-v", "--verbose",
                               dest="verbose",
                               action="store_true",
                               help="Run in verbose mode")

    verbose_group.add_argument("-q", "--quiet",
                               dest="quiet",
                               action="store_true",
                               help="Run in quiet mode")

    object_group = parser.add_argument_group(style.host("Object management "
                                                        "options"),
                                             "Options to create and destroy "
                                             "Spark cluster objects")

    object_mutex_group = object_group.add_mutually_exclusive_group()

    object_mutex_group.add_argument("--create",
                                    metavar="MODE",
                                    nargs=1,
                                    action="store",
                                    help="Create the cluster object with the "
                                    "given mode")

    object_mutex_group.add_argument("--delete",
                                    dest="delete",
                                    action="store_true",
                                    help="Remove all files used by the cluster")

    object_group.add_argument("--properties",
                              dest="properties",
                              nargs=1,
                              action="store",
                              help="File containing the properties to be used "
                              "(INI file). Applies only to --create")

    object_group.add_argument("--nodes",
                              metavar="MACHINELIST",
                              dest="nodes",
                              nargs=1,
                              action="store",
                              help="Use the nodes in MACHINELIST file. Applies"
                                   " only to --create")

    object_group.add_argument("--hid",
                              metavar="ID",
                              dest="hid",
                              nargs=1,
                              action="store",
                              help="Link to the Hadoop cluster with the given "
                                   "identifier. Applies only to --create")

    object_group.add_argument("--bootstrap",
                              metavar="SPARK_TAR",
                              nargs=1,
                              action="store",
                              help="Install Spark in the cluster nodes taking"
                                   " into account the specified properties.\n"
                                   "SPARK_TAR defines the path of the .tgz "
                                   "file containing Spark binaries.")

    actions = parser.add_argument_group(style.host("Spark actions"),
                                        "Actions to execute in the Spark "
                                        "cluster. Several options can be "
                                        "indicated at the same time.\n"
                                        "The order of execution is fixed no "
                                        "matter the order used in the "
                                        "arguments: it follows the order\n"
                                        "of the options.")

    actions.add_argument("--initialize",
                         dest="initialize",
                         nargs="?",
                         metavar="feeling_lucky",
                         const="no_default",
                         default=None,
                         action="store",
                         help="Initialize cluster: copy configuration If "
                              "nothing is specified, automatic\n"
                              "configuration just concerns needed"
                              "parameters and minimum tuning.\n"
                              "If the option 'feeling_lucky' is used then "
                              "some tuning is performed. Note that\n"
                              "the chosen options may not be optimal for all "
                              "scenarios.")

    actions.add_argument("--changeconf",
                         action="store",
                         nargs="+",
                         metavar=("[CONF_FILE] NAME=VALUE", "NAME=VALUE"),
                         help="Change given configuration variables")

    actions.add_argument("--getconf",
                         action="store",
                         nargs="+",
                         metavar="PARAM_NAME",
                         help="Get the values of the specified variables")

    actions.add_argument("--start",
                         dest="start",
                         action="store_true",
                         help="Start the master and slaves")

    exec_group = actions.add_mutually_exclusive_group()

    exec_group.add_argument("--java_job",
                            action="store",
                            nargs="+",
                            metavar=("LOCAL_JOB_PATH", "PARAM"),
                            help="Copy the Java job to the cluster and execute"
                                 " it with the specified parameters")

    exec_group.add_argument("--scala_job",
                            action="store",
                            nargs="+",
                            metavar=("LOCAL_JOB_PATH", "PARAM"),
                            help="Copy the Scala job to the cluster and execute"
                                 " it with the specified parameters")

    exec_group.add_argument("--py_job",
                            action="store",
                            nargs="+",
                            metavar=("LOCAL_JOB_PATH", "PARAM"),
                            help="Copy the Python job to the cluster and "
                                 "execute it with the specified parameters")

    exec_group.add_argument("--shell",
                            action="store",
                            nargs="?",
                            const="ipython",
                            metavar="LANGUAGE",
                            help="Start a shell in the given language "
                                 "(IPython if nothing specified)")

    actions.add_argument("--stop",
                         dest="stop",
                         action="store_true",
                         help="Stop the master and slaves")

    actions.add_argument("--clean",
                         dest="clean",
                         action="store_true",
                         help="Remove files created by Spark")

    exec_opts = parser.add_argument_group(style.host("Execution options"),
                                          "Parameters for the execution of "
                                          "jobs. Apply only to --scala_job, "
                                          "--py_job and --shell")

    exec_opts.add_argument("--node",
                           action="store",
                           nargs=1,
                           metavar="NODE",
                           help="Node where the action will be executed")

    exec_opts.add_argument("--lib_paths",
                           action="store",
                           nargs="+",
                           metavar="LIB_JARS",
                           help="A list of libraries to be used in job "
                                "execution")

    exec_opts.add_argument("--main_class",
                           action="store",
                           nargs=1,
                           metavar="CLASS",
                           help="The class to be executed. Only applies to "
                                "--scala_job")

    exec_opts.add_argument("--exec_params",
                           action="store",
                           nargs="+",
                           metavar="PARAM",
                           help="The options for job execution. Options should"
                                " be written without the \"--\" prefix (e.g., "
                                "--exec_params driver-memory=1g)")

    return parser.parse_args()

if __name__ == "__main__":

    args = parse_args()

    # Get id
    if args.id:
        sc_id = int(args.id[0])
    else:
        if args.create:
            sc_id = generate_new_id(SparkCluster.get_cluster_type())
        else:
            sc_id = get_default_id(SparkCluster.get_cluster_type())
            if not sc_id:
                logger.error("There is no available cluster. You must create a"
                             " new one")
                sys.exit(os.EX_DATAERR)

        logger.debug("Using id = " + str(sc_id) + " (SPARK)")

    verbose = not args.quiet

    # Create or load object
    if args.create:

        if cluster_exists(SparkCluster.get_cluster_type(), sc_id):
            logger.error("There is a Spark cluster with that id. You must "
                         "remove it before or chose another id")
            sys.exit(os.EX_DATAERR)

        if args.create[0].upper() == "STANDALONE":
            mode = STANDALONE_MODE
        elif args.create[0].upper() == "YARN":
            mode = YARN_MODE
        else:
            logger.error("Unrecognized mode. It should be either STANDALONE or"
                         "YARN.")
            sys.exit(os.EX_DATAERR)

        if not (args.nodes or args.hid):
            logger.error("Nodes in the cluster must be specified either "
                         "directly or indirectly through a Hadoop cluster.")
            sys.exit(os.EX_USAGE)

        hosts = hw_manager.get_hosts_list(args.nodes[0]) if args.nodes else None

        if args.hid:
            hc_id = args.hid[0]

            # Deserialize HadoopCluster and link it
            hc = deserialize_cluster(HadoopCluster.get_cluster_type(), hc_id)
            link_to_hadoop_cluster(SparkCluster.get_cluster_type(), sc_id,
                                   hc_id)
        else:
            hc = None

        props = args.properties[0] if args.properties else None

        sc = SparkCluster(mode, config_file=props, hosts=hosts,
                          hadoop_cluster=hc)

    elif args.delete:

        # Clean
        sc = deserialize_cluster(SparkCluster.get_cluster_type(), sc_id)
        if sc.initialized:
            logger.warn("The cluster needs to be cleaned before removed.")
            sc.clean()

        # Remove hc dump file
        logger.info("Removing hc dump file from cluster")
        remove_cluster(SparkCluster.get_cluster_type(), sc_id)

        sys.exit(os.EX_OK)
    else:

        # Deserialize
        sc = deserialize_cluster(SparkCluster.get_cluster_type(), sc_id)

        # Print cluster info
        logger.info("spark_id = %d -> %s" % (sc_id, sc))

    # Execute options
    if args.bootstrap:
        sc.bootstrap(args.bootstrap[0])

    if args.initialize:
        if args.initialize == "feeling_lucky":
            sc.initialize(default_tuning=True)
        else:
            sc.initialize()
        changed = True

    if args.changeconf:
        if "=" in args.changeconf[0]:
            conf_file = None
            list_params = args.changeconf
        else:
            conf_file = args.changeconf[0]
            list_params = args.changeconf[1:]

        params = {}
        for assig in list_params:
            parts = assig.split("=")
            params[parts[0]] = parts[1]

        if conf_file:
            sc.change_conf(params, conf_file=conf_file)
        else:
            sc.change_conf(params)

        changed = True

    if args.getconf:
        params = sc.get_conf(args.getconf)
        for p in params:
            print p + " = " + params[p]

    if args.start:
        sc.start()

    if args.shell or args.java_job or args.scala_job or args.py_job:
        node_host = Host(args.node[0]) if args.node else None
        lib_paths = args.lib_paths if args.lib_paths else None

        if args.exec_params:
            exec_params = ["--" + p.replace("=", " ") for p in args.exec_params]
        else:
            exec_params = None

        if args.shell:
            sc.start_shell(language=args.shell,
                           node=node_host,
                           exec_params=exec_params)
        elif args.java_job or args.scala_job or args.py_job:
            if args.java_job or args.scala_job:
                sjob = args.java_job if args.java_job else args.scala_job

                main_class = args.main_class[0] if args.main_class else None
                job = JavaOrScalaSparkJob(job_path=sjob[0],
                                          exec_params=exec_params,
                                          app_params=sjob[1:],
                                          lib_paths=lib_paths,
                                          main_class=main_class)
            else:  # elif args.py_job:
                job = PythonSparkJob(job_path=args.py_job[0],
                                     exec_params=exec_params,
                                     app_params=args.py_job[1:],
                                     lib_paths=lib_paths)

            sc.execute_job(job, verbose=verbose, node=node_host)
            if job.success:
                print "Job finished successfully"
            else:
                print "Job finished with errors"

    else:
        if args.node:
            logger.warn("--node only applies to --java_job, --scala_job, "
                        "--py_job or --shell. Ignoring argument")
        if args.exec_params:
            logger.warn("--exec_params only applies to --java_job, "
                        "--scala_job, --py_job or --shell. Ignoring argument")
        if args.lib_paths:
            logger.warn("--lib_paths only applies to --java_job, --scala_job, "
                        "--py_job or --shell. Ignoring argument")

    if args.stop:
        sc.stop()

    if args.clean:
        sc.clean()

    serialize_cluster(SparkCluster.get_cluster_type(), sc_id, sc)
