#!/usr/bin/env python

import os
import sys
import threading
from argparse import ArgumentParser, RawTextHelpFormatter

from execo.action import Get, Put, TaktukRemote
from execo.host import Host
from execo.log import style
from execo.process import SshProcess
from execo_engine import logger

from bigdata_dpy.fw.hadoop import HadoopCluster, HadoopV2Cluster
from bigdata_dpy.fw.hadoop.objects import HadoopJarJob
from bigdata_dpy.util import hw_manager
from bigdata_dpy.util.serialization import generate_new_id, \
    get_default_id, cluster_exists, deserialize_cluster, remove_cluster, \
    serialize_cluster


def parse_args():

    prog = "hdpy"
    description = "This tool helps you to manage a Hadoop cluster in Grid5000."
    parser = ArgumentParser(prog=prog,
                            description=description,
                            formatter_class=RawTextHelpFormatter,
                            add_help=False)

    actions = parser.add_argument_group(style.host("General options"),
                                        "Options to be used generally "
                                        "with Hadoop actions.")

    actions.add_argument("-h", "--help",
                         action="help",
                         help="Show this help message and exit")

    actions.add_argument("--id",
                         action="store",
                         nargs=1,
                         metavar="ID",
                         help="The identifier of the cluster. If not indicated"
                                ", last used cluster will be used (if any)")

    actions.add_argument("--node",
                         action="store",
                         nargs=1,
                         metavar="NODE",
                         help="Node where the action will be executed. Applies"
                                " only to --execute and --jarjob")

    actions.add_argument("--libjars",
                         action="store",
                         nargs="+",
                         metavar="LIB_JARS",
                         help="A list of libraries to be used in job execution"
                                ". Applies only to --jarjob")

    verbose_group = actions.add_mutually_exclusive_group()

    verbose_group.add_argument("-v", "--verbose",
                               dest="verbose",
                               action="store_true",
                               help="Run in verbose mode")

    verbose_group.add_argument("-q", "--quiet",
                               dest="quiet",
                               action="store_true",
                               help="Run in quiet mode")

    object_group = parser.add_argument_group(style.host("Object management "
                                                        "options"),
                                             "Options to create and destroy "
                                             "hadoop cluster objects.")

    object_mutex_group = object_group.add_mutually_exclusive_group()

    object_mutex_group.add_argument("--create",
                                    metavar="MACHINELIST",
                                    nargs=1,
                                    action="store",
                                    help="Create the cluster object with the "
                                    "nodes in MACHINELIST file")

    object_mutex_group.add_argument("--delete",
                                    dest="delete",
                                    action="store_true",
                                    help="Remove all files used by the cluster")

    object_group.add_argument("--version",
                              dest="version",
                              nargs=1,
                              action="store",
                              help="Hadoop version to be used. It should be "
                                   "specified at cluster creation time.\n"
                                   "If nothing is specified, a cluster "
                                   "compliant with version 1.x is created.\n"
                                   "Applies only to --create")

    object_group.add_argument("--properties",
                              dest="properties",
                              nargs=1,
                              action="store",
                              help="File containing the properties to be used "
                              "(INI file). Applies only to --create")

    object_group.add_argument("--bootstrap",
                              metavar="HADOOP_TAR",
                              nargs=1,
                              action="store",
                              help="Install Hadoop in the cluster nodes taking"
                                   " into account the specified properties.\n"
                                   "HADOOP_TAR defines the path of the .tar.gz"
                                   " file containing Hadoop binaries.")

    actions = parser.add_argument_group(style.host("Hadoop actions"),
                                        "Actions to execute in the Hadoop "
                                        "cluster. Several options can be "
                                        "indicated at the same time.\n"
                                        "The order of execution is fixed no "
                                        "matter the order used in the "
                                        "arguments: it follows the order\n"
                                        "of the options.")

    actions.add_argument("--initialize",
                         dest="initialize",
                         nargs="?",
                         metavar="feeling_lucky",
                         const="no_default",
                         default=None,
                         action="store",
                         help="Initialize cluster: copy configuration and "
                              "format dfs. If nothing is specified,\n"
                              "automatic configuration just concerns needed"
                              "parameters and minimum tuning.\n"
                              "If the option 'feeling_lucky' is used then "
                              "some tuning is performed. Note that\n"
                              "the chosen options may not be optimal for all "
                              "scenarios.")

    actions.add_argument("--changeconf",
                         action="store",
                         nargs="+",
                         metavar=("[CONF_FILE] NAME=VALUE", "NAME=VALUE"),
                         help="Change given configuration variables")

    start_mutex_group = actions.add_mutually_exclusive_group()

    start_mutex_group.add_argument("--start",
                                   dest="start",
                                   action="store_true",
                                   help="Start all the services")

    start_mutex_group.add_argument("--start_hdfs",
                                   dest="start_hdfs",
                                   action="store_true",
                                   help="Start the NameNode and the DataNodes")

    start_mutex_group.add_argument("--start_mr",
                                   dest="start_mr",
                                   action="store_true",
                                   help="Start the JobTracker and the "
                                        "TaskTrackers")

    start_mutex_group.add_argument("--start_yarn",
                                   dest="start_yarn",
                                   action="store_true",
                                   help="Start the ResourceManager and "
                                        "NodeManagers")

    actions.add_argument("--putindfs",
                         action="store",
                         nargs="+",
                         metavar="PATH",
                         help="Copy a set of local paths into the remote path "
                         "in dfs")

    actions.add_argument("--getfromdfs",
                         action="store",
                         nargs=2,
                         metavar=("DFS_PATH", "LOCAL_PATH"),
                         help="Copy a remote path in dfs into the specified "
                         "local path")

    actions.add_argument("--execute",
                         action="store",
                         nargs=1,
                         metavar="COMMAND",
                         help="Execute a Hadoop command")

    actions.add_argument("--jarjob",
                         action="store",
                         nargs="+",
                         metavar=("LOCAL_JAR_PATH", "PARAM"),
                         help="Copy the jar file and execute it with the "
                         "specified parameters")

    actions.add_argument("--copyhistory",
                         action="store",
                         nargs="+",
                         metavar=("LOCAL_PATH", "JOB_ID"),
                         help="Copy history to the specified path.\n"
                         "If a list of job ids is given, just copy the "
                         "stats of those jobs.")

    actions.add_argument("--stop",
                         dest="stop",
                         action="store_true",
                         help="Stop all the services")

    actions.add_argument("--clean",
                         dest="clean",
                         action="store_true",
                         help="Remove Hadoop logs and clean the dfs")

    queries = parser.add_argument_group(style.host("Hadoop queries"),
                                        "Set of queries that can be executed "
                                        "in Hadoop.")

    queries.add_argument("--state",
                         action="store",
                         nargs="?",
                         const="general",
                         choices=["general", "files", "dfs",
                                  "dfsblocks", "mrjobs"],
                         help="Show the cluster state. The output depends on "
                         "optional argument:\n"
                         "  general    Show general cluster state"
                         " (default option)\n"
                         "  files      Show dfs file hierarchy\n" +
                         "  dfs        Show filesystem state\n" +
                         "  dfsblocks  Show dfs blocks information\n" +
                         "  mrjobs     Show mapreduce state\n")

    queries.add_argument("--getconf",
                         action="store",
                         nargs="+",
                         metavar="PARAM_NAME",
                         help="Get the values of the specified variables")

    return parser.parse_args()


def print_state(section="general"):

    if section == "general":

        logger.info("-"*55)
        logger.info(style.user2("Hadoop Cluster with ID " + str(hc_id)))
        logger.info(style.user1("    Version: ") + hc.get_version())
        logger.info(style.user1("    Master: ") + str(hc.master))
        logger.info(style.user1("    Hosts: ") + str(hc.hosts))
        logger.info(style.user1("    Topology: "))
        for h in hc.hosts:
            logger.info("        " + str(h) + " -> " +
                        str(hc.topology.get_rack(h)))
        if hc.initialized:
            if hc.running_dfs:
                logger.info("HDFS is " + style.user3("running"))
            else:
                logger.info("HDFS is " + style.user3("stopped"))

            if hc.get_major_version() < 2:
                if hc.running_map_reduce:
                    logger.info("MapReduce is " + style.user3("running"))
                else:
                    logger.info("MapReduce is " + style.user3("stopped"))
            else:
                if hc.running_yarn:
                    logger.info("YARN is " + style.user3("running"))
                else:
                    logger.info("YARN is " + style.user3("stopped"))
        else:
            logger.info("The cluster is not " + style.user3("initialized"))
        logger.info("-"*55)

    elif section == "files":
        if hc.get_major_version() == 2:
            (stdout, stderr) = hc.execute("fs -ls -R /", verbose=False)
        else:
            (stdout, stderr) = hc.execute("fs -lsr /", verbose=False)
        print ""
        for line in stdout.splitlines():
            if line.startswith("d") or line.startswith("-"):
                print line
        print ""

        size = None
        if hc.get_major_version() == 2:
            (stdout, stderr) = hc.execute("fs -du -s /", verbose=False)
            for line in stdout.splitlines():
                if "WARN" not in line and "warning" not in line:
                    try:
                        pos = line.rfind("\t")
                        size = int(line[:pos])
                        break
                    except:
                        size = None
        else:
            (stdout, stderr) = hc.execute("fs -dus /", verbose=False)
            pos = stdout.rfind("\t")
            size = int(stdout[pos + 1:])

        if size:
            human_readable_size = ""
            if 1024 < size < 1048576:
                human_readable_size = " (%.1f KB)" % (float(size)/1024)
            elif 1048576 < size < 1073741824:
                human_readable_size = " (%.1f MB)" % (float(size)/1048576)
            elif size > 1073741824:
                human_readable_size = " (%.1f GB)" % (float(size)/1073741824)

            print "Total Size = " + str(size) + human_readable_size + "\n"

    elif section == "dfs":
        (stdout, stderr) = hc.execute("dfsadmin -report", verbose=False)
        print ""
        for line in stdout.splitlines():
            if "WARN fs.FileSystem" not in line:
                print line
        print ""

    elif section == "dfsblocks":
        (stdout, stderr) = hc.execute("fsck -blocks", verbose=False)
        print ""
        print stdout
        print ""

    elif section == "mrjobs":
        (stdout, stderr) = hc.execute("job -list all", verbose=False)
        print ""
        print stdout
        print ""


def putindfs(hc, action_args):
    local_paths = action_args[:-1]
    dest = action_args[-1]

    for f in local_paths:
        if not os.path.exists(f):
            logger.error("Local path " + f + " does not exist")
            sys.exit(os.EX_NOINPUT)

    # Create dest directories if needed
    if hc.get_major_version() == 2:
        hc.execute("fs -mkdir -p " + dest, verbose=False)

    # Define and create temp dir
    tmp_dir = "/tmp/hg5k_dest"
    hosts = hc.hosts
    action_remove = TaktukRemote("rm -rf " + tmp_dir, hosts)
    action_remove.run()
    action_create = TaktukRemote("mkdir -p " + tmp_dir, hosts)
    action_create.run()

    def copy_function(host, files_to_copy):
        action_copy = Put([host], files_to_copy, tmp_dir)
        action_copy.run()

        for f in files_to_copy:
            dest_suffix = os.path.basename(os.path.normpath(f))
            src_file = os.path.join(tmp_dir, dest_suffix)

            if hc.get_major_version() == 2:
                hc.execute("fs -put " + src_file + " " + dest,
                           host, True, False)
            else:
                hc.execute("fs -put " + src_file + " " +
                           os.path.join(dest, dest_suffix),
                           host, True, False)

    # Assign files to hosts
    files_per_host = [[]] * len(hosts)
    for idx in range(0, len(hosts)):
        files_per_host[idx] = local_paths[idx::len(hosts)]

    # Create threads and launch them
    logger.info("Copying files in parallel into " + str(len(hosts)) +
                " hosts")

    threads = []
    for idx, h in enumerate(hosts):
        if files_per_host[idx]:
            t = threading.Thread(target=copy_function,
                                 args=(h, files_per_host[idx]))
            t.start()
            threads.append(t)

    # Wait for the threads to finish
    for t in threads:
        t.join()


def getfromdfs(hc, action_args):
    remote_path = action_args[0]
    local_path = action_args[1]

    tmp_dir = "/tmp"
    # Remove file in tmp dir if exists
    proc = SshProcess("rm -rf " +
                      os.path.join(tmp_dir, os.path.basename(remote_path)),
                      hc.master)
    proc.run()

    # Get files in master
    hc.execute("fs -get " + remote_path + " " + tmp_dir, verbose=False)

    # Copy files from master
    action = Get([hc.master],
                 [os.path.join(tmp_dir, os.path.basename(remote_path))],
                 local_path)
    action.run()

if __name__ == "__main__":

    args = parse_args()

    changed = False

    # Get id
    if args.id:
        hc_id = int(args.id[0])
    else:
        if args.create:
            hc_id = generate_new_id(HadoopCluster.get_cluster_type())
        else:
            hc_id = get_default_id(HadoopCluster.get_cluster_type())
            if not hc_id:
                logger.error("There is no available cluster. You must create a"
                             " new one")
                sys.exit(os.EX_DATAERR)

        logger.debug("Using id = " + str(hc_id) + " (HADOOP)")

    verbose = not args.quiet

    # Check node specification
    node_host = None
    if args.node:
        if not (args.execute or args.jarjob):
            logger.warn("--node only applies to --execute or --jarjob")
        else:
            node_host = Host(args.node[0])

    # Create or load object
    if args.create:

        if cluster_exists(HadoopCluster.get_cluster_type(), hc_id):
            logger.error("There is a hadoop cluster with that id. You must "
                         "remove it before or chose another id")
            sys.exit(os.EX_DATAERR)

        hosts = hw_manager.get_hosts_list(args.create[0])

        if args.version:
            if args.version[0] == "0" or args.version[0].startswith("0."):
                hadoop_class = HadoopCluster
            elif args.version[0] == "1" or args.version[0].startswith("1."):
                hadoop_class = HadoopCluster
            elif args.version[0] == "2" or args.version[0].startswith("2."):
                hadoop_class = HadoopV2Cluster
            else:
                logger.error("Unknown hadoop version")
                sys.exit(os.EX_DATAERR)
        else:
            hadoop_class = HadoopCluster

        if args.properties:
            hc = hadoop_class(hosts, None, args.properties[0])
        else:
            hc = hadoop_class(hosts)
        changed = True

    else:
        if args.version:
            logger.warn("--version only applies to cluster creation")
        if args.properties:
            logger.warn("--properties only applies to cluster creation")

        if args.delete:

            # Clean
            hc = deserialize_cluster(HadoopCluster.get_cluster_type(), hc_id)
            if hc.initialized:
                logger.warn("The cluster needs to be cleaned before removed.")
                hc.clean()

            # Remove hc dump file
            logger.info("Removing hc dump file from cluster")
            remove_cluster(HadoopCluster.get_cluster_type(), hc_id)

            sys.exit(os.EX_OK)
        else:
            # Deserialize
            hc = deserialize_cluster(HadoopCluster.get_cluster_type(), hc_id)

        # Print cluster info
        logger.info("hadoop_id = %d -> %s" % (hc_id, hc))

    # Execute options
    if args.bootstrap:
        f = args.bootstrap[0]
        if not os.path.exists(f):
            logger.error("Hadoop distribution file " + f + " does not exist")
            sys.exit(os.EX_NOINPUT)
        hc.bootstrap(args.bootstrap[0])
        changed = True

    if args.initialize:
        if args.initialize == "feeling_lucky":
            hc.initialize(default_tuning=True)
        else:
            hc.initialize()
        changed = True

    if args.changeconf:
        if "=" in args.changeconf[0]:
            conf_file = None
            list_params = args.changeconf
        else:
            conf_file = args.changeconf[0]
            list_params = args.changeconf[1:]

        params = {}
        for assig in list_params:
            parts = assig.split("=")
            params[parts[0]] = parts[1]

        if conf_file:
            hc.change_conf(params, conf_file=conf_file)
        else:
            hc.change_conf(params)

        changed = True

    if args.getconf:
        params = hc.get_conf(args.getconf)
        for p in params:
            print p + " = " + params[p]

    if args.start:
        hc.start_and_wait()
        changed = True

    if args.start_hdfs:
        hc.start_dfs()
        changed = True

    if args.start_mr:
        if hc.get_major_version() >= 2:
            logger.error("MapReduce daemons are only part of Hadoop versions "
                        "0.*. and 1.*")
            sys.exit(os.EX_USAGE)
        hc.start_mapreduce()
        changed = True

    if args.start_yarn:
        if hc.get_major_version() != 2:
            logger.error("YARN is only available in Hadoop 2.*")
            sys.exit(os.EX_USAGE)
        hc.start_yarn()
        changed = True

    if args.putindfs:
        putindfs(hc, args.putindfs)
        changed = True

    if args.getfromdfs:
        getfromdfs(hc, args.getfromdfs)

    if args.execute:
        if node_host:
            hc.execute(args.execute[0], node_host, verbose=verbose)
        else:
            hc.execute(args.execute[0], verbose=verbose)

        changed = True

    if args.jarjob:
        if not node_host:
            node_host = None
        if args.libjars:
            libjars = args.libjars
        else:
            libjars = None

        job = HadoopJarJob(args.jarjob[0], args.jarjob[1:], libjars)

        hc.execute_job(job, verbose=verbose, node=node_host)
        if job.success:
            print "Job with id " + job.job_id + " finished successfully"
        else:
            print "Job finished with errors"

        changed = True

    if args.copyhistory:
        hc.copy_history(args.copyhistory[0], args.copyhistory[1:])

    if args.stop:
        hc.stop()
        changed = True

    if args.clean:
        hc.clean()
        changed = True

    if args.state:
        print_state(args.state)

    if changed:
        serialize_cluster(HadoopCluster.get_cluster_type(), hc_id, hc)
