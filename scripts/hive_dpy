#!/usr/bin/env python

import os
import sys
from argparse import ArgumentParser, RawTextHelpFormatter

from execo import Host
from execo.log import style
from execo_engine import logger

from bigdata_dpy.fw.hadoop.cluster import HadoopCluster
from bigdata_dpy.fw.hadoop.ecosystem.hive import HiveCluster
from bigdata_dpy.util.serialization import generate_new_id, cluster_exists, \
    link_to_hadoop_cluster, deserialize_cluster, remove_cluster, \
    serialize_cluster, get_default_id

if __name__ == "__main__":

    prog = "hive_dpy"
    description = "This tool helps you to manage a Hive cluster in Grid5000."
    parser = ArgumentParser(prog=prog,
                            description=description,
                            formatter_class=RawTextHelpFormatter,
                            add_help=False)

    actions = parser.add_argument_group(style.host("General options"),
                                        "Options to be used generally "
                                        "with Hive actions.")

    actions.add_argument("-h", "--help",
                         action="help",
                         help="Show this help message and exit")

    actions.add_argument("--id",
                         action="store",
                         nargs=1,
                         metavar="ID",
                         help="The identifier of the cluster. If not indicated"
                                ", last used cluster will be used (if any)")

    verbose_group = actions.add_mutually_exclusive_group()

    verbose_group.add_argument("-v", "--verbose",
                               dest="verbose",
                               action="store_true",
                               help="Run in verbose mode")

    verbose_group.add_argument("-q", "--quiet",
                               dest="quiet",
                               action="store_true",
                               help="Run in quiet mode")

    object_group = parser.add_argument_group(style.host("Object management "
                                                        "options"),
                                             "Options to create and destroy "
                                             "Spark cluster objects")

    object_mutex_group = object_group.add_mutually_exclusive_group()

    object_mutex_group.add_argument("--create",
                                    metavar="MODE",
                                    nargs=1,
                                    action="store",
                                    help="Create the cluster object linked to "
                                         "the HadoopCluster with the given "
                                         "identifier")

    object_mutex_group.add_argument("--delete",
                                    dest="delete",
                                    action="store_true",
                                    help="Remove all files used by the cluster")

    object_group.add_argument("--properties",
                              dest="properties",
                              nargs=1,
                              action="store",
                              help="File containing the properties to be used "
                              "(INI file). Applies only to --create")

    object_group.add_argument("--bootstrap",
                              metavar="HIVE_TAR",
                              nargs=1,
                              action="store",
                              help="Install Hive in the cluster nodes taking"
                                   " into account the specified properties.\n"
                                   "HIVE_TAR defines the path of the .tar.gz "
                                   "file containing Hive binaries.")

    actions = parser.add_argument_group(style.host("Hive actions"),
                                        "Actions to execute in the Hive "
                                        "cluster. Several options can be "
                                        "indicated at the same time.\n"
                                        "The order of execution is fixed no "
                                        "matter the order used in the "
                                        "arguments: it follows the order\n"
                                        "of the options.")

    actions.add_argument("--initialize",
                         dest="initialize",
                         action="store_true",
                         help="Initialize cluster: Copy configuration")

    actions.add_argument("--start",
                         dest="start",
                         action="store_true",
                         help="Start the master and slaves")

    exec_group = actions.add_mutually_exclusive_group()

    exec_group.add_argument("--job",
                            action="store",
                            nargs="+",
                            metavar=("LOCAL_JOB_PATH", "PARAM"),
                            help="Copy the job to the cluster and execute it "
                                 "with the specified parameters")

    exec_group.add_argument("--shell",
                            action="store_true",
                            help="Start a shell session in mongodb")

    actions.add_argument("--stop",
                         dest="stop",
                         action="store_true",
                         help="Stop the master and slaves")

    actions.add_argument("--clean",
                         dest="clean",
                         action="store_true",
                         help="Remove files created by Hive")

    exec_opts = parser.add_argument_group(style.host("Execution options"),
                                          "Parameters for the execution of "
                                          "jobs. Apply only to --job, and "
                                          "--shell")

    exec_opts.add_argument("--node",
                           action="store",
                           nargs=1,
                           metavar="NODE",
                           help="Node where the action will be executed")

    exec_opts.add_argument("--exec_params",
                           action="store",
                           nargs="+",
                           metavar="PARAM",
                           help="The options for job execution. Options should"
                                " be written without the \"--\" prefix")

    args = parser.parse_args()

    # Get id
    if args.id:
        hivec_id = int(args.id[0])
    else:
        if args.create:
            hivec_id = generate_new_id(HiveCluster.get_cluster_type())
        else:
            hivec_id = get_default_id(HiveCluster.get_cluster_type())
            if not hivec_id:
                logger.error("There is no available cluster. You must create a"
                             " new one")
                sys.exit(os.EX_DATAERR)

    logger.info("Using id = " + str(hivec_id) + " (HIVE)")

    verbose = not args.quiet

    # Create or load object
    if args.create:

        if cluster_exists(HiveCluster.get_cluster_type(), hivec_id):
            logger.error("There is a Spark cluster with that id. You must "
                         "remove it before or chose another id")
            sys.exit(os.EX_DATAERR)

        # Deserialize HadoopCluster and link it
        hc_id = args.create[0]
        hc = deserialize_cluster(HadoopCluster.get_cluster_type(), hc_id)
        link_to_hadoop_cluster(HiveCluster.get_cluster_type(), hivec_id, hc_id)

        # Create cluster
        props = args.properties[0] if args.properties else None
        hivec = HiveCluster(hc, config_file=props)

    elif args.delete:

        # Clean
        hivec = deserialize_cluster(HiveCluster.get_cluster_type(), hivec_id)
        if hivec.initialized:
            logger.warn("The cluster needs to be cleaned before removed.")
            hivec.clean()

        # Remove hc dump file
        logger.info("Removing hc dump file from cluster")
        remove_cluster(HiveCluster.get_cluster_type(), hivec_id)

        sys.exit(os.EX_OK)
    else:

        # Deserialize
        hivec = deserialize_cluster(HiveCluster.get_cluster_type(), hivec_id)

    # Execute options
    if args.bootstrap:
        hivec.bootstrap(args.bootstrap[0])

    if args.initialize:
        hivec.initialize()

    if args.start:
        hivec.start()

    if args.shell or args.job:
        node_host = Host(args.node[0]) if args.node else None

        if args.exec_params:
            exec_params = ["--" + p.replace("=", " ") for p in args.exec_params]
        else:
            exec_params = None

        if args.shell:
            hivec.start_shell(node=node_host, exec_params=exec_params)
        elif args.job:
            logger.error("Not implemented yet")

    else:
        if args.node:
            logger.warn("--node only applies to --job or --shell. Ignoring "
                        "argument")
        if args.exec_params:
            logger.warn("--exec_params only applies to --job or --shell. "
                        "Ignoring argument")

    if args.stop:
        hivec.stop()

    if args.clean:
        hivec.clean()

    serialize_cluster(HiveCluster.get_cluster_type(), hivec_id, hivec)
